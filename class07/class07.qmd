---
title: "Class 7: Clustering and PCA"
author: "Emily Rodriguez"
format: pdf
---

# Clustering

First let's make up some data to cluster so we can get a feel for these methods and how to work with them.

We can use thr `rnorm()` function to get random numbers from a normal distribution around a given `mean`.

```{r}
hist(rnorm(5000, mean = 3))
```

Let's get 30 points with a mean of 3.

```{r}
tmp <-c(rnorm(30, mean = 3), rnorm(30, mean = -3))
tmp
```

Put two of these together:
```{r}
x <- cbind(x = tmp, y = rev(tmp))
plot(x)
```


## K-means clustering.

Very popular clustering method that we can use with the `kmeans()` function in base R.

```{r}
km <- kmeans(x, centers = 2)
km
```


# Generate some example data for clustering

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, 3))
x <- data.frame(x = tmp, y = rev(tmp))
plot(x)
```


> Q. How many points are in each cluster?

> Q. What `component` of your result object details?

      - cluster size?
```{r}
km$size
```

      - cluster?
```{r}
km$cluster
```

      - cluster center?
```{r}
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points.

```{r}
plot(x, col = km$cluster)
points(km$centers, col = "blue", pch = 15, cex = 3)
```

> Q. Let's cluster into 3 groups or same `x` data and make a plot.

```{r}
km <- kmeans(x, centers = 3)
plot(x, col = km$cluster)
```

# Hierarchical Cluster 

We can use the `hclust()` function for Hierarchical Clustering. Unlike `kmeans()`, where we could just pass in our data as input, we need to give `hclust()` a "distance matrix". 


We will use the `dist()` function to start with

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```


```{r}
plot(hc)
```


I can now "cut my tree with the `cutree()` to yield a cluster membership vector.

```{r}
grps <- cutree(hc, h = 8)
grps
```

You can also tell `cutree()` to cut where it yields "k" groups.

```{r}
cutree(hc, k = 2)
```

```{r}
plot(x, col = grps)
```

# Principle Component Analysis (PCA)

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
x
```

> Q1. How many rows and columns are in your new data frame named `x`? What R functions could you use to answer this question?

```{r}
dim(x)
```


```{r}
x <- x[,-1]
head(x)
```

>Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

    I prefer using the `read.csv()` function because the `rownames()` function removes      England from the data and giving us only three rows instead of four.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

>Q3: Changing what optional argument in the above barplot() function results in the following plot?

    Changing the argument `beside` to `F` will result in the following bar plot.

```{r}
barplot(as.matrix(x), beside= F, col=rainbow(nrow(x)))
```


>Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

    Some countries lie on the y-axis and some on the x-axis. It means that both             countries use the same amount of product.


```{r}
pairs(x, col=rainbow(10), pch=16)
```

>Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

    The main difference is the blue point.

>Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

    The main PCA function in base R is called `prcomp()` it expects the transpose of our     data.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

```{r}
attributes(pca)
```


```{r}
pca$x
```


```{r}
plot(pca$x[,1], pca$x[,2], col = c("orange", "red", "blue", "darkgreen"), pch = 16)
```






